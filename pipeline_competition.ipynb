{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers import load_csv_data\n",
    "\n",
    "data_path = \"./data/train.csv\"\n",
    "test_path = \"./data/test.csv\"\n",
    "sample_path = \"./data/sample-submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30) (250000,) (250000,)\n",
      "[ 1.38470e+02  5.16550e+01  9.78270e+01  2.79800e+01  9.10000e-01\n",
      "  1.24711e+02  2.66600e+00  3.06400e+00  4.19280e+01  1.97760e+02\n",
      "  1.58200e+00  1.39600e+00  2.00000e-01  3.26380e+01  1.01700e+00\n",
      "  3.81000e-01  5.16260e+01  2.27300e+00 -2.41400e+00  1.68240e+01\n",
      " -2.77000e-01  2.58733e+02  2.00000e+00  6.74350e+01  2.15000e+00\n",
      "  4.44000e-01  4.60620e+01  1.24000e+00 -2.47500e+00  1.13497e+02] 1.0 100000\n"
     ]
    }
   ],
   "source": [
    "labels, features, ids = load_csv_data(data_path, sub_sample=False)\n",
    "\n",
    "print(features.shape, labels.shape, ids.shape)\n",
    "print(features[0], labels[0], ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessor\n",
    "\n",
    "prep = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(features).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 31) (200000,)\n",
      "(50000, 31) (50000,)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "from helpers import train_test_split\n",
    "features_tr, features_te, labels_tr, labels_te = train_test_split(labels, features, 0.8, seed=432)\n",
    "# features_tr, labels_tr = prep.remove_outlier(labels_tr, features_tr)\n",
    "features_tr = prep.process_train(features_tr)\n",
    "features_te = prep.process_train(features_te)\n",
    "\n",
    "print(features_tr.shape, labels_tr.shape)\n",
    "print(features_te.shape, labels_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Least square GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamme: 0.1 \ttrain: [loss=0.34346, acc=0.7411]        \ttest: [loss=0.34310, accuracy=0.7411]\n",
      "gamme: 0.03 \ttrain: [loss=0.35539, acc=0.7310]        \ttest: [loss=0.35507, accuracy=0.7310]\n",
      "gamme: 0.01 \ttrain: [loss=0.37450, acc=0.7190]        \ttest: [loss=0.37410, accuracy=0.7190]\n",
      "gamme: 0.003 \ttrain: [loss=0.41859, acc=0.6857]        \ttest: [loss=0.41781, accuracy=0.6857]\n",
      "gamme: 0.001 \ttrain: [loss=0.45683, acc=0.6550]        \ttest: [loss=0.45618, accuracy=0.6550]\n"
     ]
    }
   ],
   "source": [
    "from helpers import train_test_split, compute_loss, accuracy_score,make_prediction\n",
    "from implementations import mean_squared_error_gd\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(labels_tr, features_tr, ratio=0.8, seed=42)\n",
    "\n",
    "max_iter = 100\n",
    "gammas = [1e-1, 3e-2, 1e-2, 3e-3, 1e-3]\n",
    "# gammas = [1e-1]\n",
    "mse = np.zeros((len(gammas), 2))\n",
    "\n",
    "initial_w = np.zeros((x_tr.shape[1]))\n",
    "\n",
    "\n",
    "for i, gamma in enumerate(gammas):\n",
    "\n",
    "    w, l_tr = mean_squared_error_gd(y_tr, x_tr, initial_w, max_iter, gamma)\n",
    "    l_te = compute_loss(y_te, x_te, w)\n",
    "\n",
    "    mse[i,:] = [l_tr, l_te]\n",
    "    acc_tr = accuracy_score(y_tr, make_prediction(x_tr @ w))\n",
    "    acc_te = accuracy_score(y_te, make_prediction(x_te @ w))\n",
    "\n",
    "    print(f\"gamme: {gamma} \\ttrain: [loss={mse[i,0]:.5f}, acc={acc_te:.4f}]\\\n",
    "        \\ttest: [loss={mse[i,1]:.5f}, accuracy={acc_te:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc=0.74337, test acc=0.74242\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy on test set\n",
    "\n",
    "gamma = 3e-2\n",
    "max_iter = 500\n",
    "initial_w = np.zeros((features_tr.shape[1]))\n",
    "w, l_tr = mean_squared_error_gd(labels_tr, features_tr, initial_w, max_iter, gamma)\n",
    "\n",
    "acc_tr = accuracy_score(labels_tr, make_prediction(features_tr @ w))\n",
    "acc_te = accuracy_score(labels_te, make_prediction(features_te @ w))\n",
    "\n",
    "print(f\"train acc={acc_tr:.5f}, test acc={acc_te:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Least Square SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamme: 0.0100 \ttrain: [loss=0.43122, acc=0.7182]        \ttest: [loss=0.43349, accuracy=0.7182]\n",
      "gamme: 0.0030 \ttrain: [loss=0.36289, acc=0.7353]        \ttest: [loss=0.36262, accuracy=0.7353]\n",
      "gamme: 0.0010 \ttrain: [loss=0.34932, acc=0.7411]        \ttest: [loss=0.34955, accuracy=0.7411]\n",
      "gamme: 0.0003 \ttrain: [loss=0.34299, acc=0.7455]        \ttest: [loss=0.34309, accuracy=0.7455]\n",
      "gamme: 0.0001 \ttrain: [loss=0.34105, acc=0.7439]        \ttest: [loss=0.34109, accuracy=0.7439]\n"
     ]
    }
   ],
   "source": [
    "from helpers import train_test_split, compute_loss, accuracy_score,make_prediction\n",
    "from implementations import mean_squared_error_sgd\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(labels_tr, features_tr, ratio=0.8, seed=42)\n",
    "\n",
    "max_iter = 3\n",
    "gammas = [1e-2, 3e-3, 1e-3, 3e-4, 1e-4]\n",
    "# gammas = np.logspace(-0.8,-2, 5)\n",
    "mse = np.zeros((len(gammas), 2))\n",
    "\n",
    "initial_w = np.zeros((x_tr.shape[1]))\n",
    "\n",
    "\n",
    "for i, gamma in enumerate(gammas):\n",
    "\n",
    "    w, l_tr = mean_squared_error_sgd(y_tr, x_tr, initial_w, max_iter, gamma)\n",
    "    l_te = compute_loss(y_te, x_te, w)\n",
    "\n",
    "    mse[i,:] = [l_tr, l_te]\n",
    "    acc_tr = accuracy_score(y_tr, make_prediction(x_tr @ w))\n",
    "    acc_te = accuracy_score(y_te, make_prediction(x_te @ w))\n",
    "\n",
    "    print(f\"gamme: {gamma:.4f} \\ttrain: [loss={mse[i,0]:.5f}, acc={acc_te:.4f}]\\\n",
    "        \\ttest: [loss={mse[i,1]:.5f}, accuracy={acc_te:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc=0.74216, test acc=0.74156\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy on test set\n",
    "\n",
    "gamma = 3e-4\n",
    "max_iter = 3\n",
    "initial_w = np.zeros((features_tr.shape[1]))\n",
    "w, l_tr = mean_squared_error_sgd(labels_tr, features_tr, initial_w, max_iter, gamma)\n",
    "\n",
    "acc_tr = accuracy_score(labels_tr, make_prediction(features_tr @ w))\n",
    "acc_te = accuracy_score(labels_te, make_prediction(features_te @ w))\n",
    "\n",
    "print(f\"train acc={acc_tr:.5f}, test acc={acc_te:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Least Squares with normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss=0.33958, test loss=1.23150\n",
      "train acc=0.74506, test acc=0.54426\n"
     ]
    }
   ],
   "source": [
    "from implementations import least_squares\n",
    "\n",
    "# No hyperparameter to chosse\n",
    "\n",
    "w, l_tr = least_squares(labels_tr, features_tr)\n",
    "l_te = compute_loss(labels_te, features_te, w)\n",
    "acc_tr = accuracy_score(labels_tr, make_prediction(features_tr @ w))\n",
    "acc_te = accuracy_score(labels_te, make_prediction(features_te @ w))\n",
    "\n",
    "print(f\"train loss={l_tr:.5f}, test loss={l_te:.5f}\")\n",
    "print(f\"train acc={acc_tr:.5f}, test acc={acc_te:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Ridge Regression with normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_: 0.000010 \ttrain: [loss=0.33998, acc=0.74262]        \ttest: [loss=0.34019, accuracy=0.74262]\n",
      "lambda_: 0.000028 \ttrain: [loss=0.34010, acc=0.74235]        \ttest: [loss=0.34028, accuracy=0.74235]\n",
      "lambda_: 0.000077 \ttrain: [loss=0.34016, acc=0.74228]        \ttest: [loss=0.34032, accuracy=0.74228]\n",
      "lambda_: 0.000215 \ttrain: [loss=0.34019, acc=0.74233]        \ttest: [loss=0.34034, accuracy=0.74233]\n",
      "lambda_: 0.000599 \ttrain: [loss=0.34021, acc=0.74230]        \ttest: [loss=0.34033, accuracy=0.74230]\n",
      "lambda_: 0.001668 \ttrain: [loss=0.34024, acc=0.74228]        \ttest: [loss=0.34031, accuracy=0.74228]\n",
      "lambda_: 0.004642 \ttrain: [loss=0.34041, acc=0.74213]        \ttest: [loss=0.34038, accuracy=0.74213]\n",
      "lambda_: 0.012915 \ttrain: [loss=0.34136, acc=0.74250]        \ttest: [loss=0.34114, accuracy=0.74250]\n",
      "lambda_: 0.035938 \ttrain: [loss=0.34518, acc=0.74067]        \ttest: [loss=0.34474, accuracy=0.74067]\n",
      "lambda_: 0.100000 \ttrain: [loss=0.35472, acc=0.73420]        \ttest: [loss=0.35425, accuracy=0.73420]\n"
     ]
    }
   ],
   "source": [
    "from helpers import train_test_split, compute_loss, accuracy_score,make_prediction\n",
    "from implementations import ridge_regression\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(labels_tr, features_tr, ratio=0.8, seed=42)\n",
    "\n",
    "max_iter = 3\n",
    "lambdas = np.logspace(-5, -1, 10)\n",
    "mse = np.zeros((len(lambdas), 2))\n",
    "\n",
    "\n",
    "for i, lambda_ in enumerate(lambdas):\n",
    "\n",
    "    w, l_tr = ridge_regression(y_tr, x_tr, lambda_)\n",
    "    l_te = compute_loss(y_te, x_te, w)\n",
    "\n",
    "    mse[i,:] = [l_tr, l_te]\n",
    "    acc_tr = accuracy_score(y_tr, make_prediction(x_tr @ w))\n",
    "    acc_te = accuracy_score(y_te, make_prediction(x_te @ w))\n",
    "\n",
    "    print(f\"lambda_: {lambda_:.6f} \\ttrain: [loss={mse[i,0]:.5f}, acc={acc_te:.5f}]\\\n",
    "        \\ttest: [loss={mse[i,1]:.5f}, accuracy={acc_te:.5f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this model is clearly underfitting, need features engineering before word on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamme: 1.5849 \ttrain: [loss=0.70072, acc=0.6973]        \ttest: [loss=0.69790, accuracy=0.6973]\n",
      "gamme: 0.7943 \ttrain: [loss=0.49838, acc=0.7490]        \ttest: [loss=0.49725, accuracy=0.7490]\n",
      "gamme: 0.3981 \ttrain: [loss=0.50020, acc=0.7490]        \ttest: [loss=0.49919, accuracy=0.7490]\n",
      "gamme: 0.1995 \ttrain: [loss=0.50546, acc=0.7459]        \ttest: [loss=0.50460, accuracy=0.7459]\n",
      "gamme: 0.1000 \ttrain: [loss=0.51395, acc=0.7404]        \ttest: [loss=0.51333, accuracy=0.7404]\n"
     ]
    }
   ],
   "source": [
    "from helpers import train_test_split, sigmoid, compute_loss_logistic, accuracy_score, make_prediction\n",
    "from implementations import logistic_regression\n",
    "\n",
    "# convert labels from {-1,1} to {0,1}\n",
    "labels_tr01 = 0.5 + labels_tr / 2.\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(labels_tr01, features_tr, ratio=0.8, seed=42)\n",
    "\n",
    "max_iter = 300\n",
    "gammas = np.logspace(0.2, -1, 5)\n",
    "# gammas = np.logspace(-0.8,-2, 5)\n",
    "mse = np.zeros((len(gammas), 2))\n",
    "\n",
    "initial_w = np.zeros((x_tr.shape[1]))\n",
    "\n",
    "\n",
    "for i, gamma in enumerate(gammas):\n",
    "\n",
    "    w, l_tr = logistic_regression(y_tr, x_tr, initial_w, max_iter, gamma)\n",
    "    l_te = compute_loss_logistic(y_te, x_te, w)\n",
    "\n",
    "    mse[i,:] = [l_tr, l_te]\n",
    "    acc_tr = accuracy_score(y_tr, make_prediction(sigmoid(x_tr @ w), logistic=True, zero_one=True))\n",
    "    acc_te = accuracy_score(y_te, make_prediction(sigmoid(x_te @ w), logistic=True, zero_one=True))\n",
    "\n",
    "    print(f\"gamme: {gamma:.4f} \\ttrain: [loss={l_tr:.5f}, acc={acc_te:.4f}]\\\n",
    "        \\ttest: [loss={l_te:.5f}, accuracy={acc_te:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc=0.70296, test acc=0.70134\n"
     ]
    }
   ],
   "source": [
    "from helpers import train_test_split, sigmoid, compute_loss_logistic, accuracy_score, make_prediction\n",
    "from implementations import logistic_regression\n",
    "\n",
    "# convert labels from {-1,1} to {0,1}\n",
    "labels_tr01 = 0.5 + labels_tr / 2.\n",
    "labels_te01 = 0.5 + labels_te / 2.\n",
    "\n",
    "max_iter = 300\n",
    "gamma = 0.4\n",
    "initial_w = np.zeros((x_tr.shape[1]))\n",
    "\n",
    "w, l_tr = logistic_regression(labels_tr, features_tr, initial_w, max_iter, gamma)\n",
    "l_te = compute_loss_logistic(labels_te, features_te, w)\n",
    "\n",
    "acc_tr = accuracy_score(labels_tr01, make_prediction(sigmoid(features_tr @ w), logistic=True, zero_one=True))\n",
    "acc_te = accuracy_score(labels_te01, make_prediction(sigmoid(features_te @ w), logistic=True, zero_one=True))\n",
    "\n",
    "print(f\"train acc={acc_tr:.5f}, test acc={acc_te:.5f}\")\n",
    "# The results are not as good as in training, possibly caused by outliers in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Regularized Logisic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import train_test_split, sigmoid, compute_loss_logistic, accuracy_score, make_prediction\n",
    "from implementations import reg_logistic_regression\n",
    "\n",
    "# convert labels from {-1,1} to {0,1}\n",
    "labels_tr01 = 0.5 + labels_tr / 2.\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(labels_tr01, features_tr, ratio=0.8, seed=42)\n",
    "\n",
    "max_iter = 300\n",
    "lambda_ = 0.1\n",
    "gammas = np.logspace(0.2, -1, 5)\n",
    "# gammas = np.logspace(-0.8,-2, 5)\n",
    "mse = np.zeros((len(gammas), 2))\n",
    "\n",
    "initial_w = np.zeros((x_tr.shape[1]))\n",
    "\n",
    "\n",
    "for i, gamma in enumerate(gammas):\n",
    "\n",
    "    w, l_tr = reg_logistic_regression(y_tr, x_tr, lambda_, initial_w, max_iter, gamma)\n",
    "    l_te = compute_loss_logistic(y_te, x_te, w)\n",
    "\n",
    "    mse[i,:] = [l_tr, l_te]\n",
    "    acc_tr = accuracy_score(y_tr, make_prediction(sigmoid(x_tr @ w), logistic=True, zero_one=True))\n",
    "    acc_te = accuracy_score(y_te, make_prediction(sigmoid(x_te @ w), logistic=True, zero_one=True))\n",
    "\n",
    "    print(f\"gamme: {gamma:.4f} \\ttrain: [loss={l_tr:.5f}, acc={acc_te:.4f}]\\\n",
    "        \\ttest: [loss={l_te:.5f}, accuracy={acc_te:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: we should not expect a ggod result with penalty term, the model is suffering from underfitting! Polynomial features could be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "_ , features_submit, ids_submit = load_csv_data(test_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model and train it on the whole dataset\n",
    "# TODO\n",
    "\n",
    "# Make prediction\n",
    "pred = None # TODO\n",
    "\n",
    "\n",
    "# EXample with MSE GD\n",
    "from implementations import mean_squared_error_gd\n",
    "gamma = 3e-2\n",
    "max_iter = 2000\n",
    "initial_w = np.zeros((features.shape[1]))\n",
    "w, l_tr = mean_squared_error_gd(labels, features, initial_w, max_iter, gamma)\n",
    "\n",
    "from helpers import make_prediction\n",
    "pred = make_prediction(features_submit @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file \n",
    "\n",
    "from helpers import create_csv_submission\n",
    "\n",
    "create_csv_submission(ids_submit, pred, sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9df5590c36242588aa2ec08b48d5460772f3434d8fd6d76e09f7949a0a7cd2ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
