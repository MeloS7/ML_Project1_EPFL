{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers import load_csv_data\n",
    "\n",
    "data_path = \"./data/train.csv\"\n",
    "test_path = \"./data/test.csv\"\n",
    "sample_path = \"./data/sample-submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30) (250000,) (250000,)\n",
      "[ 1.38470e+02  5.16550e+01  9.78270e+01  2.79800e+01  9.10000e-01\n",
      "  1.24711e+02  2.66600e+00  3.06400e+00  4.19280e+01  1.97760e+02\n",
      "  1.58200e+00  1.39600e+00  2.00000e-01  3.26380e+01  1.01700e+00\n",
      "  3.81000e-01  5.16260e+01  2.27300e+00 -2.41400e+00  1.68240e+01\n",
      " -2.77000e-01  2.58733e+02  2.00000e+00  6.74350e+01  2.15000e+00\n",
      "  4.44000e-01  4.60620e+01  1.24000e+00 -2.47500e+00  1.13497e+02] 1.0 100000\n"
     ]
    }
   ],
   "source": [
    "labels, features, ids = load_csv_data(data_path, sub_sample=False)\n",
    "\n",
    "print(features.shape, labels.shape, ids.shape)\n",
    "print(features[0], labels[0], ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessor\n",
    "\n",
    "prep = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(features).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "#TRAIN# (250000, 11) # (250000, 30)\n",
      "(200000, 162) (200000,)\n",
      "(50000, 162) (50000,)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "from helpers import train_test_split\n",
    "features = prep.remove_outlier_features(features)\n",
    "features = prep.process(features, poly_degree=5)\n",
    "features_tr, features_te, labels_tr, labels_te = train_test_split(labels, features, 0.8, seed=432)\n",
    "# features_tr = prep.remove_outlier_features(features_tr)\n",
    "# features_te = prep.remove_outlier_features(features_te)\n",
    "# features_tr = prep.process_train(features_tr, apply_mapping=False, poly_degree=5)\n",
    "# features_te = prep.process_test(features_te)\n",
    "\n",
    "print(features_tr.shape, labels_tr.shape)\n",
    "print(features_te.shape, labels_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(features_tr.max())\n",
    "print(features_tr.min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Least square GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.01 \ttrain: [loss=0.40128, acc=0.6938]        \ttest: [loss=0.40070, accuracy=0.6938]\n",
      "gamma: 0.01778279410038923 \ttrain: [loss=0.38843, acc=0.6968]        \ttest: [loss=0.38793, accuracy=0.6968]\n",
      "gamma: 0.03162277660168379 \ttrain: [loss=0.37589, acc=0.7099]        \ttest: [loss=0.37548, accuracy=0.7099]\n",
      "gamma: 0.05623413251903491 \ttrain: [loss=0.36454, acc=0.7192]        \ttest: [loss=0.36425, accuracy=0.7192]\n",
      "gamma: 0.1 \ttrain: [loss=0.35446, acc=0.7251]        \ttest: [loss=0.35435, accuracy=0.7251]\n"
     ]
    }
   ],
   "source": [
    "from helpers import train_test_split, compute_loss, accuracy_score,make_prediction\n",
    "from implementations import mean_squared_error_gd\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(labels_tr, features_tr, ratio=0.8, seed=42)\n",
    "\n",
    "max_iter = 100\n",
    "# gammas = [1e-1, 3e-2, 1e-2, 3e-3, 1e-3]\n",
    "gammas = np.logspace(-2,-1, 5)\n",
    "mse = np.zeros((len(gammas), 2))\n",
    "\n",
    "initial_w = np.zeros((x_tr.shape[1]))\n",
    "\n",
    "\n",
    "for i, gamma in enumerate(gammas):\n",
    "\n",
    "    w, l_tr = mean_squared_error_gd(y_tr, x_tr, initial_w, max_iter, gamma)\n",
    "    l_te = compute_loss(y_te, x_te, w)\n",
    "\n",
    "    mse[i,:] = [l_tr, l_te]\n",
    "    acc_tr = accuracy_score(y_tr, make_prediction(x_tr @ w))\n",
    "    acc_te = accuracy_score(y_te, make_prediction(x_te @ w))\n",
    "\n",
    "    print(f\"gamma: {gamma} \\ttrain: [loss={mse[i,0]:.5f}, acc={acc_te:.4f}]\\\n",
    "        \\ttest: [loss={mse[i,1]:.5f}, accuracy={acc_te:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc=0.72828, test acc=0.72784\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy on test set\n",
    "\n",
    "gamma = 3e-2\n",
    "max_iter = 500\n",
    "initial_w = np.zeros((features_tr.shape[1]))\n",
    "w, l_tr = mean_squared_error_gd(labels_tr, features_tr, initial_w, max_iter, gamma)\n",
    "\n",
    "acc_tr = accuracy_score(labels_tr, make_prediction(features_tr @ w))\n",
    "acc_te = accuracy_score(labels_te, make_prediction(features_te @ w))\n",
    "\n",
    "print(f\"train acc={acc_tr:.5f}, test acc={acc_te:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Least Square SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamme: 0.0100 \ttrain: [loss=0.33470, acc=0.7572]        \ttest: [loss=0.33509, accuracy=0.7572]\n",
      "gamme: 0.0030 \ttrain: [loss=0.33267, acc=0.7523]        \ttest: [loss=0.33319, accuracy=0.7523]\n",
      "gamme: 0.0010 \ttrain: [loss=0.33819, acc=0.7421]        \ttest: [loss=0.33878, accuracy=0.7421]\n",
      "gamme: 0.0003 \ttrain: [loss=0.34642, acc=0.7312]        \ttest: [loss=0.34730, accuracy=0.7312]\n",
      "gamme: 0.0001 \ttrain: [loss=0.35332, acc=0.7238]        \ttest: [loss=0.35428, accuracy=0.7238]\n"
     ]
    }
   ],
   "source": [
    "from helpers import train_test_split, compute_loss, accuracy_score,make_prediction\n",
    "from implementations import mean_squared_error_sgd\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(labels_tr, features_tr, ratio=0.8, seed=42)\n",
    "\n",
    "max_iter = 3\n",
    "gammas = [1e-2, 3e-3, 1e-3, 3e-4, 1e-4]\n",
    "# gammas = np.logspace(-0.8,-2, 5)\n",
    "mse = np.zeros((len(gammas), 2))\n",
    "\n",
    "initial_w = np.zeros((x_tr.shape[1]))\n",
    "\n",
    "\n",
    "for i, gamma in enumerate(gammas):\n",
    "\n",
    "    w, l_tr = mean_squared_error_sgd(y_tr, x_tr, initial_w, max_iter, gamma)\n",
    "    l_te = compute_loss(y_te, x_te, w)\n",
    "\n",
    "    mse[i,:] = [l_tr, l_te]\n",
    "    acc_tr = accuracy_score(y_tr, make_prediction(x_tr @ w))\n",
    "    acc_te = accuracy_score(y_te, make_prediction(x_te @ w))\n",
    "\n",
    "    print(f\"gamma: {gamma:.4f} \\ttrain: [loss={mse[i,0]:.5f}, acc={acc_te:.4f}]\\\n",
    "        \\ttest: [loss={mse[i,1]:.5f}, accuracy={acc_te:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc=0.73146, test acc=0.73176\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy on test set\n",
    "\n",
    "gamma = 3e-4\n",
    "max_iter = 3\n",
    "initial_w = np.zeros((features_tr.shape[1]))\n",
    "w, l_tr = mean_squared_error_sgd(labels_tr, features_tr, initial_w, max_iter, gamma)\n",
    "\n",
    "acc_tr = accuracy_score(labels_tr, make_prediction(features_tr @ w))\n",
    "acc_te = accuracy_score(labels_te, make_prediction(features_te @ w))\n",
    "\n",
    "print(f\"train acc={acc_tr:.5f}, test acc={acc_te:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Least Squares with normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\epfl\\ml-cs433\\ML_Project1\\pipeline_competition.ipynb Cellule 17\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/epfl/ml-cs433/ML_Project1/pipeline_competition.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimplementations\u001b[39;00m \u001b[39mimport\u001b[39;00m least_squares\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/epfl/ml-cs433/ML_Project1/pipeline_competition.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# No hyperparameter to chosse\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/epfl/ml-cs433/ML_Project1/pipeline_competition.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m w, l_tr \u001b[39m=\u001b[39m least_squares(labels_tr, features_tr)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/epfl/ml-cs433/ML_Project1/pipeline_competition.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m l_te \u001b[39m=\u001b[39m compute_loss(labels_te, features_te, w)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/epfl/ml-cs433/ML_Project1/pipeline_competition.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m acc_tr \u001b[39m=\u001b[39m accuracy_score(labels_tr, make_prediction(features_tr \u001b[39m@\u001b[39m w))\n",
      "File \u001b[1;32md:\\epfl\\ml-cs433\\ML_Project1\\implementations.py:77\u001b[0m, in \u001b[0;36mleast_squares\u001b[1;34m(y, tx)\u001b[0m\n\u001b[0;32m     75\u001b[0m a \u001b[39m=\u001b[39m tx\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mdot(tx)\n\u001b[0;32m     76\u001b[0m b \u001b[39m=\u001b[39m tx\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mdot(y)\n\u001b[1;32m---> 77\u001b[0m w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49msolve(a, b)\n\u001b[0;32m     78\u001b[0m loss \u001b[39m=\u001b[39m compute_loss(y, tx, w)\n\u001b[0;32m     80\u001b[0m \u001b[39mreturn\u001b[39;00m w, loss\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msolve\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32md:\\apps\\anaconda3\\envs\\ml-cs433\\lib\\site-packages\\numpy\\linalg\\linalg.py:400\u001b[0m, in \u001b[0;36msolve\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m    398\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDD->D\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdd->d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    399\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 400\u001b[0m r \u001b[39m=\u001b[39m gufunc(a, b, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[0;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(r\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[1;32md:\\apps\\anaconda3\\envs\\ml-cs433\\lib\\site-packages\\numpy\\linalg\\linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSingular matrix\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "from implementations import least_squares\n",
    "\n",
    "# No hyperparameter to chosse\n",
    "\n",
    "w, l_tr = least_squares(labels_tr, features_tr)\n",
    "l_te = compute_loss(labels_te, features_te, w)\n",
    "acc_tr = accuracy_score(labels_tr, make_prediction(features_tr @ w))\n",
    "acc_te = accuracy_score(labels_te, make_prediction(features_te @ w))\n",
    "\n",
    "print(f\"train loss={l_tr:.5f}, test loss={l_te:.5f}\")\n",
    "print(f\"train acc={acc_tr:.5f}, test acc={acc_te:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Ridge Regression with normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_: 0.000010 \ttrain: [loss=0.30227, acc=0.78663]        \ttest: [loss=0.30360, accuracy=0.78663]\n",
      "lambda_: 0.000028 \ttrain: [loss=0.30406, acc=0.78443]        \ttest: [loss=0.30514, accuracy=0.78443]\n",
      "lambda_: 0.000077 \ttrain: [loss=0.30688, acc=0.78092]        \ttest: [loss=0.30774, accuracy=0.78092]\n",
      "lambda_: 0.000215 \ttrain: [loss=0.31094, acc=0.77630]        \ttest: [loss=0.31162, accuracy=0.77630]\n",
      "lambda_: 0.000599 \ttrain: [loss=0.31669, acc=0.76818]        \ttest: [loss=0.31728, accuracy=0.76818]\n",
      "lambda_: 0.001668 \ttrain: [loss=0.32413, acc=0.75800]        \ttest: [loss=0.32465, accuracy=0.75800]\n",
      "lambda_: 0.004642 \ttrain: [loss=0.33209, acc=0.74900]        \ttest: [loss=0.33253, accuracy=0.74900]\n",
      "lambda_: 0.012915 \ttrain: [loss=0.34141, acc=0.73650]        \ttest: [loss=0.34163, accuracy=0.73650]\n",
      "lambda_: 0.035938 \ttrain: [loss=0.35487, acc=0.72545]        \ttest: [loss=0.35478, accuracy=0.72545]\n",
      "lambda_: 0.100000 \ttrain: [loss=0.37297, acc=0.70998]        \ttest: [loss=0.37263, accuracy=0.70998]\n"
     ]
    }
   ],
   "source": [
    "from helpers import train_test_split, compute_loss, accuracy_score,make_prediction\n",
    "from implementations import ridge_regression\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(labels_tr, features_tr, ratio=0.8, seed=42)\n",
    "\n",
    "max_iter = 3\n",
    "lambdas = np.logspace(-5, -1, 10)\n",
    "mse = np.zeros((len(lambdas), 2))\n",
    "\n",
    "\n",
    "for i, lambda_ in enumerate(lambdas):\n",
    "\n",
    "    w, l_tr = ridge_regression(y_tr, x_tr, lambda_)\n",
    "    l_te = compute_loss(y_te, x_te, w)\n",
    "\n",
    "    mse[i,:] = [l_tr, l_te]\n",
    "    acc_tr = accuracy_score(y_tr, make_prediction(x_tr @ w))\n",
    "    acc_te = accuracy_score(y_te, make_prediction(x_te @ w))\n",
    "\n",
    "    print(f\"lambda_: {lambda_:.6f} \\ttrain: [loss={mse[i,0]:.5f}, acc={acc_te:.5f}]\\\n",
    "        \\ttest: [loss={mse[i,1]:.5f}, accuracy={acc_te:.5f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this model is clearly underfitting, need features engineering before word on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.9120 \ttrain: [loss=0.55508, acc=0.7200]        \ttest: [loss=0.55655, accuracy=0.7200]\n",
      "gamma: 0.7852 \ttrain: [loss=0.50150, acc=0.7480]        \ttest: [loss=0.50207, accuracy=0.7480]\n",
      "gamma: 0.6761 \ttrain: [loss=0.50324, acc=0.7463]        \ttest: [loss=0.50377, accuracy=0.7463]\n",
      "gamma: 0.5821 \ttrain: [loss=0.50502, acc=0.7440]        \ttest: [loss=0.50551, accuracy=0.7440]\n",
      "gamma: 0.5012 \ttrain: [loss=0.50692, acc=0.7420]        \ttest: [loss=0.50736, accuracy=0.7420]\n"
     ]
    }
   ],
   "source": [
    "from helpers import train_test_split, sigmoid, compute_loss_logistic, accuracy_score, make_prediction\n",
    "from implementations import logistic_regression\n",
    "\n",
    "# convert labels from {-1,1} to {0,1}\n",
    "labels_tr01 = 0.5 + labels_tr / 2.\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(labels_tr01, features_tr, ratio=0.8, seed=42)\n",
    "\n",
    "max_iter = 300\n",
    "gammas = np.logspace(-0.04, -0.3, 5)\n",
    "# gammas = np.logspace(-0.8,-2, 5)\n",
    "mse = np.zeros((len(gammas), 2))\n",
    "\n",
    "initial_w = np.zeros((x_tr.shape[1]))\n",
    "\n",
    "\n",
    "for i, gamma in enumerate(gammas):\n",
    "\n",
    "    w, l_tr = logistic_regression(y_tr, x_tr, initial_w, max_iter, gamma)\n",
    "    l_te = compute_loss_logistic(y_te, x_te, w)\n",
    "\n",
    "    mse[i,:] = [l_tr, l_te]\n",
    "    acc_tr = accuracy_score(y_tr, make_prediction(sigmoid(x_tr @ w), logistic=True, zero_one=True))\n",
    "    acc_te = accuracy_score(y_te, make_prediction(sigmoid(x_te @ w), logistic=True, zero_one=True))\n",
    "\n",
    "    print(f\"gamma: {gamma:.4f} \\ttrain: [loss={l_tr:.5f}, acc={acc_te:.4f}]\\\n",
    "        \\ttest: [loss={l_te:.5f}, accuracy={acc_te:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\epfl\\ml-cs433\\ML_Project1\\helpers.py:115: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc=0.65716, test acc=0.65802\n"
     ]
    }
   ],
   "source": [
    "from helpers import train_test_split, sigmoid, compute_loss_logistic, accuracy_score, make_prediction\n",
    "from implementations import logistic_regression\n",
    "\n",
    "# convert labels from {-1,1} to {0,1}\n",
    "labels_tr01 = 0.5 + labels_tr / 2.\n",
    "labels_te01 = 0.5 + labels_te / 2.\n",
    "\n",
    "max_iter = 300\n",
    "gamma = 0.7\n",
    "\n",
    "initial_w = np.zeros((x_tr.shape[1]))\n",
    "\n",
    "w, l_tr = logistic_regression(labels_tr, features_tr, initial_w, max_iter, gamma)\n",
    "l_te = compute_loss_logistic(labels_te, features_te, w)\n",
    "\n",
    "acc_tr = accuracy_score(labels_tr01, make_prediction(sigmoid(features_tr @ w), logistic=True, zero_one=True))\n",
    "acc_te = accuracy_score(labels_te01, make_prediction(sigmoid(features_te @ w), logistic=True, zero_one=True))\n",
    "\n",
    "print(f\"train acc={acc_tr:.5f}, test acc={acc_te:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Regularized Logisic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 1.0000, lambda_: 0.1000            \ttrain: [loss=0.80562, acc=0.5553]            \ttest: [loss=0.76570, accuracy=0.5553]\n",
      "gamma: 1.0000, lambda_: 0.0178            \ttrain: [loss=0.80194, acc=0.5828]            \ttest: [loss=0.76254, accuracy=0.5828]\n",
      "gamma: 1.0000, lambda_: 0.0032            \ttrain: [loss=0.70622, acc=0.6608]            \ttest: [loss=0.67664, accuracy=0.6608]\n",
      "gamma: 1.0000, lambda_: 0.0006            \ttrain: [loss=0.63472, acc=0.6887]            \ttest: [loss=0.62592, accuracy=0.6887]\n",
      "gamma: 1.0000, lambda_: 0.0001            \ttrain: [loss=0.61164, acc=0.6973]            \ttest: [loss=0.61138, accuracy=0.6973]\n",
      "gamma: 0.5623, lambda_: 0.1000            \ttrain: [loss=0.61133, acc=0.6958]            \ttest: [loss=0.59015, accuracy=0.6958]\n",
      "gamma: 0.5623, lambda_: 0.0178            \ttrain: [loss=0.57238, acc=0.7171]            \ttest: [loss=0.54822, accuracy=0.7171]\n",
      "gamma: 0.5623, lambda_: 0.0032            \ttrain: [loss=0.53356, acc=0.7339]            \ttest: [loss=0.51645, accuracy=0.7339]\n",
      "gamma: 0.5623, lambda_: 0.0006            \ttrain: [loss=0.51232, acc=0.7415]            \ttest: [loss=0.50766, accuracy=0.7415]\n",
      "gamma: 0.5623, lambda_: 0.0001            \ttrain: [loss=0.50675, acc=0.7433]            \ttest: [loss=0.50622, accuracy=0.7433]\n",
      "gamma: 0.3162, lambda_: 0.1000            \ttrain: [loss=0.61133, acc=0.6958]            \ttest: [loss=0.59015, accuracy=0.6958]\n",
      "gamma: 0.3162, lambda_: 0.0178            \ttrain: [loss=0.57238, acc=0.7169]            \ttest: [loss=0.54843, accuracy=0.7169]\n",
      "gamma: 0.3162, lambda_: 0.0032            \ttrain: [loss=0.53520, acc=0.7296]            \ttest: [loss=0.52159, accuracy=0.7296]\n",
      "gamma: 0.3162, lambda_: 0.0006            \ttrain: [loss=0.51854, acc=0.7346]            \ttest: [loss=0.51538, accuracy=0.7346]\n",
      "gamma: 0.3162, lambda_: 0.0001            \ttrain: [loss=0.51471, acc=0.7356]            \ttest: [loss=0.51432, accuracy=0.7356]\n",
      "gamma: 0.1778, lambda_: 0.1000            \ttrain: [loss=0.61133, acc=0.6958]            \ttest: [loss=0.59015, accuracy=0.6958]\n",
      "gamma: 0.1778, lambda_: 0.0178            \ttrain: [loss=0.57246, acc=0.7154]            \ttest: [loss=0.54994, accuracy=0.7154]\n",
      "gamma: 0.1778, lambda_: 0.0032            \ttrain: [loss=0.53960, acc=0.7254]            \ttest: [loss=0.53006, accuracy=0.7254]\n",
      "gamma: 0.1778, lambda_: 0.0006            \ttrain: [loss=0.52802, acc=0.7263]            \ttest: [loss=0.52593, accuracy=0.7263]\n",
      "gamma: 0.1778, lambda_: 0.0001            \ttrain: [loss=0.52560, acc=0.7262]            \ttest: [loss=0.52520, accuracy=0.7262]\n",
      "gamma: 0.1000, lambda_: 0.1000            \ttrain: [loss=0.61133, acc=0.6958]            \ttest: [loss=0.59016, accuracy=0.6958]\n",
      "gamma: 0.1000, lambda_: 0.0178            \ttrain: [loss=0.57323, acc=0.7115]            \ttest: [loss=0.55443, accuracy=0.7115]\n",
      "gamma: 0.1000, lambda_: 0.0032            \ttrain: [loss=0.54758, acc=0.7197]            \ttest: [loss=0.54147, accuracy=0.7197]\n",
      "gamma: 0.1000, lambda_: 0.0006            \ttrain: [loss=0.54038, acc=0.7206]            \ttest: [loss=0.53896, accuracy=0.7206]\n",
      "gamma: 0.1000, lambda_: 0.0001            \ttrain: [loss=0.53897, acc=0.7208]            \ttest: [loss=0.53851, accuracy=0.7208]\n"
     ]
    }
   ],
   "source": [
    "from helpers import train_test_split, sigmoid, compute_loss_logistic, accuracy_score, make_prediction\n",
    "from implementations import reg_logistic_regression\n",
    "\n",
    "# convert labels from {-1,1} to {0,1}\n",
    "labels_tr01 = 0.5 + labels_tr / 2.\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(labels_tr01, features_tr, ratio=0.8, seed=42)\n",
    "\n",
    "max_iter = 300\n",
    "lambda_ = 0.001\n",
    "lambdas = np.logspace(-1,-4, 5)\n",
    "gamma = 0.08\n",
    "gammas = np.logspace(0, -1, 5)\n",
    "# gammas = np.logspace(-0.8,-2, 5)\n",
    "mse = np.zeros((len(gammas), 2))\n",
    "\n",
    "initial_w = np.zeros((x_tr.shape[1]))\n",
    "\n",
    "\n",
    "for i, gamma in enumerate(gammas):\n",
    "    for i, lambda_ in enumerate(lambdas):\n",
    "\n",
    "        w, l_tr = reg_logistic_regression(y_tr, x_tr, lambda_, initial_w, max_iter, gamma)\n",
    "        l_te = compute_loss_logistic(y_te, x_te, w)\n",
    "\n",
    "        mse[i,:] = [l_tr, l_te]\n",
    "        acc_tr = accuracy_score(y_tr, make_prediction(sigmoid(x_tr @ w), logistic=True, zero_one=True))\n",
    "        acc_te = accuracy_score(y_te, make_prediction(sigmoid(x_te @ w), logistic=True, zero_one=True))\n",
    "\n",
    "        print(f\"gamma: {gamma:.4f}, lambda_: {lambda_:.4f}\\\n",
    "            \\ttrain: [loss={l_tr:.5f}, acc={acc_te:.4f}]\\\n",
    "            \\ttest: [loss={l_te:.5f}, accuracy={acc_te:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import train_test_split, sigmoid, compute_loss_logistic, accuracy_score, make_prediction\n",
    "from implementations import reg_logistic_regression\n",
    "\n",
    "# convert labels from {-1,1} to {0,1}\n",
    "labels_tr01 = 0.5 + labels_tr / 2.\n",
    "labels_te01 = 0.5 + labels_te / 2.\n",
    "\n",
    "max_iter = 300\n",
    "gamma = 0.7\n",
    "\n",
    "initial_w = np.zeros((x_tr.shape[1]))\n",
    "\n",
    "w, l_tr = reg_logistic_regression(labels_tr, features_tr, initial_w, max_iter, gamma)\n",
    "l_te = compute_loss_logistic(labels_te, features_te, w)\n",
    "\n",
    "acc_tr = accuracy_score(labels_tr01, make_prediction(sigmoid(features_tr @ w), logistic=True, zero_one=True))\n",
    "acc_te = accuracy_score(labels_te01, make_prediction(sigmoid(features_te @ w), logistic=True, zero_one=True))\n",
    "\n",
    "print(f\"train acc={acc_tr:.5f}, test acc={acc_te:.5f}\")\n",
    "# The results are not as good as in training, possibly caused by outliers in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "_ , features_submit, ids_submit = load_csv_data(test_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model and train it on the whole dataset\n",
    "# TODO\n",
    "\n",
    "# Make prediction\n",
    "pred = None # TODO\n",
    "\n",
    "\n",
    "# EXample with MSE GD\n",
    "from implementations import mean_squared_error_gd\n",
    "gamma = 3e-2\n",
    "max_iter = 2000\n",
    "initial_w = np.zeros((features.shape[1]))\n",
    "w, l_tr = mean_squared_error_gd(labels, features, initial_w, max_iter, gamma)\n",
    "\n",
    "from helpers import make_prediction\n",
    "pred = make_prediction(features_submit @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file \n",
    "\n",
    "from helpers import create_csv_submission\n",
    "\n",
    "create_csv_submission(ids_submit, pred, sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml-cs433')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c300989436866e0f2bdaf2ad700bc3bcca33acbeb9f30e204547f5a0aa3fb11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
